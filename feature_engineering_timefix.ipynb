{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830dd74-4b58-46b7-93ce-36b4e5335824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WINS] clip txn_amt at ±1450000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willy\\AppData\\Local\\Temp\\ipykernel_25564\\1639675063.py:333: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  tmp = (df.groupby([\"acct\", col], as_index=False).size().rename(columns={\"size\":\"cnt\"}))\n",
      "C:\\Users\\willy\\AppData\\Local\\Temp\\ipykernel_25564\\1639675063.py:335: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  wide = tmp.pivot_table(index=\"acct\", columns=col, values=\"cnt\", aggfunc=\"sum\", fill_value=0)\n",
      "C:\\Users\\willy\\AppData\\Local\\Temp\\ipykernel_25564\\1639675063.py:333: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  tmp = (df.groupby([\"acct\", col], as_index=False).size().rename(columns={\"size\":\"cnt\"}))\n",
      "C:\\Users\\willy\\AppData\\Local\\Temp\\ipykernel_25564\\1639675063.py:335: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  wide = tmp.pivot_table(index=\"acct\", columns=col, values=\"cnt\", aggfunc=\"sum\", fill_value=0)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, gc\n",
    "\n",
    "# 允許在 Jupyter/VS Code 中執行：移除 Notebook 自動注入的 -f 參數\n",
    "def _strip_notebook_argv():\n",
    "    if \"-f\" in sys.argv:\n",
    "        i = sys.argv.index(\"-f\")\n",
    "        del sys.argv[i:i+2]\n",
    "_strip_notebook_argv()\n",
    "\n",
    "# 降低隱性拷貝、節省記憶體\n",
    "pd.set_option(\"mode.copy_on_write\", True)\n",
    "\n",
    "# ================== 可調參 ==================\n",
    "WINDOW_DAYS   = 60         # 訓練/預測視窗：事件日前 N 天\n",
    "NEG_POS_RATIO = 20.0       # Hard negatives：未標示帳號抽樣比\n",
    "RANDOM_STATE  = 42\n",
    "DRIFT_TOPK    = 0\n",
    "\n",
    "# 預設路徑（可用 CLI 覆寫）\n",
    "TX_PATH    = \"acct_transaction.csv\"\n",
    "ALERT_PATH = \"acct_alert.csv\"\n",
    "PRED_PATH  = \"acct_predict.csv\"\n",
    "\n",
    "OUT_TRAIN = \"features_train.csv\"   # acct,label,is_unlabeled,<features...>\n",
    "OUT_PRED  = \"features_pred.csv\"    # acct,<features...>\n",
    "OUT_META  = \"features_meta.json\"   # feature_cols、winsorize caps 等\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "# =============== 低記憶體 CSV 讀取器 ===============\n",
    "def read_csv_safely(path, usecols=None, dtype_hint=None, prefer_arrow=True):\n",
    "    \"\"\"\n",
    "    先嘗試 Arrow 後端（省 RAM），失敗再退回 C engine + dtype hint。\n",
    "    \"\"\"\n",
    "    if prefer_arrow:\n",
    "        try:\n",
    "            import pyarrow  # noqa: F401\n",
    "            df = pd.read_csv(\n",
    "                path,\n",
    "                usecols=usecols,\n",
    "                engine=\"pyarrow\",\n",
    "                dtype_backend=\"pyarrow\"\n",
    "            )\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"[INFO] Arrow 讀取不可用，改用 C engine（原因：{type(e).__name__}）\")\n",
    "\n",
    "    # 備援：C engine + 明確 dtype + 省記憶體旗標\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        usecols=usecols,\n",
    "        dtype=(dtype_hint or {}),\n",
    "        low_memory=True,\n",
    "        memory_map=True,\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============== 小工具 ===============\n",
    "def safe_group_apply(gb, func):\n",
    "    try:\n",
    "        return gb.apply(func, include_groups=False)\n",
    "    except TypeError:\n",
    "        return gb.apply(func)\n",
    "\n",
    "def _parse_date_col_to_int_day(s: pd.Series):\n",
    "    # 支援「整數天序」或「日期字串/yyyymmdd」\n",
    "    if pd.api.types.is_integer_dtype(s) or pd.api.types.is_float_dtype(s):\n",
    "        day = pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "        return day, False, None\n",
    "    s_str = s.astype(str)\n",
    "    dt = pd.to_datetime(s_str, errors=\"coerce\")\n",
    "    if dt.isna().all():\n",
    "        mask8 = s_str.str.len().eq(8) & s_str.str.isnumeric()\n",
    "        dt2 = pd.to_datetime(s_str.where(mask8, None), format=\"%Y%m%d\", errors=\"coerce\")\n",
    "        dt = dt.fillna(dt2)\n",
    "    if dt.notna().sum() == 0:\n",
    "        return pd.Series([pd.NA]*len(s), dtype=\"Int64\"), False, None\n",
    "    dt = dt.dt.floor(\"D\")\n",
    "    base = dt.min()\n",
    "    day = (dt - base).dt.days.astype(\"Int64\")\n",
    "    return day, True, base\n",
    "\n",
    "def signed_log1p(x: pd.Series) -> pd.Series:\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "\n",
    "def compute_caps(df: pd.DataFrame, cols, lo_q=0.005, hi_q=0.995):\n",
    "    caps = {}\n",
    "    for c in cols:\n",
    "        v = pd.to_numeric(df[c], errors=\"coerce\").dropna()\n",
    "        if len(v) == 0:\n",
    "            continue\n",
    "        lo = float(np.quantile(v, lo_q)) if lo_q is not None else None\n",
    "        hi = float(np.quantile(v, hi_q)) if hi_q is not None else None\n",
    "        caps[c] = (lo, hi)\n",
    "    return caps\n",
    "\n",
    "def apply_caps(df: pd.DataFrame, caps: dict):\n",
    "    for c, (lo, hi) in caps.items():\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        if lo is not None:\n",
    "            df[c] = np.maximum(df[c], lo)\n",
    "        if hi is not None:\n",
    "            df[c] = np.minimum(df[c], hi)\n",
    "\n",
    "def entropy_from_counts(counts: np.ndarray) -> float:\n",
    "    counts = counts.astype(float)\n",
    "    s = counts.sum()\n",
    "    if s <= 0:\n",
    "        return 0.0\n",
    "    p = counts / s\n",
    "    p = p[p > 0]\n",
    "    return float(-(p * np.log(p)).sum())\n",
    "\n",
    "\n",
    "# =============== 讀檔 + 正規化（免大規模字串運算；不做去重以避 OOM） ===============\n",
    "def read_and_normalize(tx_path, alert_path, pred_path, inference_only=False):\n",
    "    # --- 先讀表頭，避免 usecols 指到不存在欄位 ---\n",
    "    head_tx = pd.read_csv(tx_path, nrows=0)\n",
    "    tx_cols_all = set(head_tx.columns.tolist())\n",
    "\n",
    "    tx_want = [\n",
    "        \"from_acct\",\"to_acct\",\n",
    "        \"txn_date\",\"txn_date_raw\",\n",
    "        \"txn_time\",\"txn_amt\",\n",
    "        \"is_self_txn\",\"from_acct_type\",\"to_acct_type\",\n",
    "        \"currency_type\",\"channel_type\",\n",
    "    ]\n",
    "    tx_usecols = [c for c in tx_want if c in tx_cols_all]\n",
    "\n",
    "    tx_dtype_hint = {\n",
    "        \"from_acct\": \"string\",\n",
    "        \"to_acct\": \"string\",\n",
    "        \"txn_time\": \"string\",\n",
    "        \"txn_amt\": \"float32\",\n",
    "        \"is_self_txn\": \"string\",\n",
    "        \"from_acct_type\": \"string\",\n",
    "        \"to_acct_type\": \"string\",\n",
    "        \"currency_type\": \"string\",\n",
    "        \"channel_type\": \"string\",\n",
    "        # txn_date/txn_date_raw 讓後續自行轉 Int64\n",
    "    }\n",
    "\n",
    "    tx = read_csv_safely(tx_path, usecols=tx_usecols, dtype_hint=tx_dtype_hint, prefer_arrow=True)\n",
    "\n",
    "    # ------- alert / pred 同樣容錯 -------\n",
    "    if not inference_only:\n",
    "        try:\n",
    "            head_alert = pd.read_csv(alert_path, nrows=0)\n",
    "            alert_cols_all = set(head_alert.columns.tolist())\n",
    "        except Exception:\n",
    "            alert_cols_all = set()\n",
    "    else:\n",
    "        alert_cols_all = set()\n",
    "\n",
    "    if not inference_only:\n",
    "        alert_want = [\"acct\",\"event_date\",\"event_date_raw\"]\n",
    "        alert_usecols  = [c for c in alert_want if c in alert_cols_all] or [\"acct\",\"event_date\"]\n",
    "        alert_dtype_hint = {\"acct\":\"string\",\"event_date\":\"string\",\"event_date_raw\":\"string\"}\n",
    "        alert = read_csv_safely(alert_path, usecols=alert_usecols, dtype_hint=alert_dtype_hint, prefer_arrow=True)\n",
    "    else:\n",
    "        alert = pd.DataFrame(columns=[\"acct\",\"event_date\"])\n",
    "\n",
    "    pred = read_csv_safely(pred_path, usecols=[\"acct\"], dtype_hint={\"acct\":\"string\"}, prefer_arrow=True)\n",
    "\n",
    "    # 若 txn_time 缺欄位，補空欄（後面時間轉分鐘會處理 NaN）\n",
    "    if \"txn_time\" not in tx.columns:\n",
    "        tx[\"txn_time\"] = pd.Series([None]*len(tx), dtype=\"string\")\n",
    "\n",
    "    #  不再去重，官方說明重複屬於資料特性；且 drop_duplicates 在 400+ 萬列會 OOM\n",
    "\n",
    "    # 帳號字串化\n",
    "    for c in [\"from_acct\",\"to_acct\"]:\n",
    "        if c in tx.columns: tx[c] = tx[c].astype(\"string\").str.strip()\n",
    "    if \"acct\" in alert.columns:\n",
    "        alert[\"acct\"] = alert[\"acct\"].astype(\"string\").str.strip()\n",
    "    pred[\"acct\"]  = pred[\"acct\"].astype(\"string\").str.strip()\n",
    "\n",
    "    # 交易日 / 事件日 → 整數日序\n",
    "    if \"txn_date_raw\" in tx.columns and tx[\"txn_date_raw\"].notna().any():\n",
    "        tx[\"txn_day\"], tx_dt, tx_base = _parse_date_col_to_int_day(tx[\"txn_date_raw\"])\n",
    "    elif \"txn_date\" in tx.columns and tx[\"txn_date\"].notna().any():\n",
    "        tx[\"txn_day\"], tx_dt, tx_base = _parse_date_col_to_int_day(tx[\"txn_date\"])\n",
    "    else:\n",
    "        raise ValueError(\"找不到交易日欄位，需 txn_date_raw 或 txn_date\")\n",
    "\n",
    "    if not inference_only:\n",
    "        if \"event_date_raw\" in alert.columns and alert[\"event_date_raw\"].notna().any():\n",
    "            alert[\"event_day\"], al_dt, al_base = _parse_date_col_to_int_day(alert[\"event_date_raw\"])\n",
    "        elif \"event_date\" in alert.columns and alert[\"event_date\"].notna().any():\n",
    "            alert[\"event_day\"], al_dt, al_base = _parse_date_col_to_int_day(alert[\"event_date\"])\n",
    "        else:\n",
    "            raise ValueError(\"找不到事件日欄位，需 event_date_raw 或 event_date\")\n",
    "    else:\n",
    "        al_dt = al_base = None\n",
    "\n",
    "    # 對齊日序基準（若兩者都是日期型）\n",
    "    if not inference_only and isinstance(tx_base, pd.Timestamp) and isinstance(al_base, pd.Timestamp):\n",
    "        shift = (al_base - tx_base).days\n",
    "        alert[\"event_day\"] = (alert[\"event_day\"].astype(\"Int64\") + shift).astype(\"Int64\")\n",
    "\n",
    "    # 時間轉分鐘 / 5 分鐘桶 / 日間密度\n",
    "    def _to_min_of_day(s):\n",
    "        s = str(s)\n",
    "        if \":\" in s:\n",
    "            hh, mm, *_ = s.split(\":\"); return int(hh)*60 + int(mm)\n",
    "        if s.isdigit():\n",
    "            return int(s[:2])*60 + int(s[2:]) if len(s)==4 else int(s)\n",
    "        return np.nan\n",
    "\n",
    "    tx[\"min_of_day\"] = tx[\"txn_time\"].astype(\"string\").apply(_to_min_of_day).astype(\"Int64\")\n",
    "    tx[\"min5_bin\"]   = (tx[\"min_of_day\"] // 5) * 5\n",
    "    tx[\"is_night\"]   = ((tx[\"min_of_day\"]>=22*60) | (tx[\"min_of_day\"]<6*60)).astype(\"Int64\")\n",
    "    tx[\"is_peak\"]    = (((tx[\"min_of_day\"]>=9*60) & (tx[\"min_of_day\"]<12*60)) |\n",
    "                        ((tx[\"min_of_day\"]>=14*60) & (tx[\"min_of_day\"]<17*60))).astype(\"Int64\")\n",
    "\n",
    "    # ========= 類別正規化（避免大字串運算造成 OOM） =========\n",
    "    # is_self_txn → 小整數（int8）\n",
    "    _map_self = {\"Y\": 1, \"N\": 0, \"UNK\": -1}\n",
    "    tx[\"is_self_txn_num\"] = (\n",
    "        tx[\"is_self_txn\"].astype(\"string\").map(_map_self).fillna(-1).astype(\"int8\")\n",
    "    )\n",
    "\n",
    "    # *_acct_type：避免 .str 操作，直接轉成數字再判斷 == 1\n",
    "    def _bank_flag(df, col):\n",
    "        if col not in df.columns:\n",
    "            return pd.Series(np.zeros(len(df), dtype=np.int8))\n",
    "        s = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int16\")\n",
    "        return (s == 1).astype(\"int8\")\n",
    "\n",
    "    tx[\"is_our_bank_from\"] = _bank_flag(tx, \"from_acct_type\")\n",
    "    tx[\"is_our_bank_to\"]   = _bank_flag(tx, \"to_acct_type\")\n",
    "\n",
    "    # 用完就釋放原始欄位\n",
    "    for _c in (\"from_acct_type\", \"to_acct_type\"):\n",
    "        if _c in tx.columns:\n",
    "            tx.drop(columns=[_c], inplace=True)\n",
    "\n",
    "    # currency / channel：改用 category 省 RAM（避免 .str.*）\n",
    "    if \"currency_type\" in tx.columns:\n",
    "        cur = tx[\"currency_type\"].astype(\"category\")\n",
    "        cats = pd.Index(cur.cat.categories.astype(str))\n",
    "        keep_up = cats.str.upper().isin([\"TWD\", \"USD\"])\n",
    "        keep_set = set(cats[keep_up])\n",
    "        tx[\"currency_bucket\"] = pd.Categorical(\n",
    "            np.where(cur.isin(keep_set), cur.astype(str), \"OTHER\")\n",
    "        )\n",
    "        tx.drop(columns=[\"currency_type\"], inplace=True)\n",
    "\n",
    "    if \"channel_type\" in tx.columns:\n",
    "        ch = tx[\"channel_type\"]\n",
    "        # 修正棄用：使用 isinstance(ch.dtype, pd.CategoricalDtype)\n",
    "        if not isinstance(ch.dtype, pd.CategoricalDtype):\n",
    "            ch = ch.astype(\"category\")\n",
    "        # 只有在尚未包含 \"UNK\" 類別時才 add_categories\n",
    "        if \"UNK\" in list(ch.cat.categories):\n",
    "            ch_filled = ch.fillna(\"UNK\")\n",
    "        else:\n",
    "            ch_filled = ch.cat.add_categories([\"UNK\"]).fillna(\"UNK\")\n",
    "        tx[\"channel_type\"] = ch_filled\n",
    "\n",
    "    # Winsorize 金額極端值（依全資料估）\n",
    "    abs_amt = pd.to_numeric(tx[\"txn_amt\"], errors=\"coerce\").abs()\n",
    "    if abs_amt.notna().any():\n",
    "        cap = float(np.quantile(abs_amt.dropna().to_numpy(), 0.995))\n",
    "        tx[\"txn_amt\"] = np.clip(pd.to_numeric(tx[\"txn_amt\"], errors=\"coerce\"), -cap, cap).astype(\"float32\")\n",
    "        print(f\"[WINS] clip txn_amt at ±{cap:.2f}\")\n",
    "\n",
    "    # ---------- 兩視角長表（省記憶體版） ----------\n",
    "    cols_extra = [\"min_of_day\",\"min5_bin\",\"is_night\",\"is_peak\"]\n",
    "\n",
    "    payer = tx[[\"from_acct\",\"to_acct\",\"txn_day\",\"txn_amt\",\"is_self_txn_num\",\"channel_type\",\n",
    "                \"currency_bucket\",\"is_our_bank_from\"] + cols_extra].copy()\n",
    "    payer.rename(columns={\"from_acct\":\"acct\",\"to_acct\":\"counterparty\",\"is_our_bank_from\":\"is_our_bank\"}, inplace=True)\n",
    "    payer[\"amt_out\"] = payer[\"txn_amt\"].astype(\"float32\")\n",
    "    payer[\"amt_in\"]  = np.float32(0.0)\n",
    "\n",
    "    payee = tx[[\"to_acct\",\"from_acct\",\"txn_day\",\"txn_amt\",\"is_self_txn_num\",\"channel_type\",\n",
    "                \"currency_bucket\",\"is_our_bank_to\"] + cols_extra].copy()\n",
    "    payee.rename(columns={\"to_acct\":\"acct\",\"from_acct\":\"counterparty\",\"is_our_bank_to\":\"is_our_bank\"}, inplace=True)\n",
    "    payee[\"amt_out\"] = np.float32(0.0)\n",
    "    payee[\"amt_in\"]  = payee[\"txn_amt\"].astype(\"float32\")\n",
    "\n",
    "    for df in (payer, payee):\n",
    "        for c in [\"acct\",\"counterparty\",\"channel_type\",\"currency_bucket\"]:\n",
    "            df[c] = df[c].astype(\"category\")\n",
    "        for c in [\"txn_day\",\"min_of_day\",\"min5_bin\"]:\n",
    "            df[c] = df[c].astype(\"int32\")\n",
    "        for c in [\"is_night\",\"is_peak\",\"is_self_txn_num\",\"is_our_bank\"]:\n",
    "            df[c] = df[c].astype(\"int8\")\n",
    "        df[\"txn_amt\"] = df[\"txn_amt\"].astype(\"float32\")\n",
    "\n",
    "    tx_long = pd.concat([payer, payee], axis=0, ignore_index=True, copy=False, sort=False)\n",
    "    tx_long.dropna(subset=[\"acct\",\"txn_day\"], inplace=True)\n",
    "    tx_long[\"acct\"] = tx_long[\"acct\"].astype(str).str.strip()\n",
    "    tx_long[\"txn_day\"] = tx_long[\"txn_day\"].astype(int)\n",
    "    del payer, payee; gc.collect()\n",
    "\n",
    "    return tx, alert, pred, tx_long\n",
    "\n",
    "\n",
    "# =============== 特徵（穩定款） ===============\n",
    "def agg_features(df):\n",
    "    if len(df)==0: return pd.DataFrame(columns=[\"acct\"])\n",
    "    g = df.groupby(\"acct\", as_index=False)\n",
    "    base = g.agg(\n",
    "        tx_cnt=(\"txn_day\",\"size\"),\n",
    "        in_cnt=(\"amt_in\",  lambda s: (s>0).sum()),\n",
    "        out_cnt=(\"amt_out\", lambda s: (s>0).sum()),\n",
    "        amt_in_sum=(\"amt_in\",\"sum\"),\n",
    "        amt_out_sum=(\"amt_out\",\"sum\"),\n",
    "        amt_abs_mean=(\"txn_amt\", lambda s: np.mean(np.abs(s))),\n",
    "        amt_abs_std=(\"txn_amt\",  lambda s: np.std(np.abs(s))),\n",
    "        amt_max=(\"txn_amt\",\"max\"),\n",
    "        active_days=(\"txn_day\", lambda s: pd.Series(s).nunique()),\n",
    "        uniq_ctp=(\"counterparty\", pd.Series.nunique),\n",
    "        self_ratio=(\"is_self_txn_num\", lambda s: (s==1).mean() if len(s)>0 else 0.0),\n",
    "        ourbank_ratio=(\"is_our_bank\",\"mean\"),\n",
    "        night_ratio=(\"is_night\",\"mean\"),\n",
    "        peak_ratio=(\"is_peak\",\"mean\"),\n",
    "    )\n",
    "    base[\"net_flow\"] = base[\"amt_in_sum\"] - base[\"amt_out_sum\"]\n",
    "    base[\"in_out_ratio\"] = np.where(base[\"out_cnt\"]>0, base[\"in_cnt\"]/base[\"out_cnt\"], 0.0)\n",
    "    return base\n",
    "\n",
    "def wide_count(df, col, prefix):\n",
    "    if col not in df.columns or len(df)==0: return pd.DataFrame(columns=[\"acct\"])\n",
    "    tmp = (df.groupby([\"acct\", col], as_index=False).size().rename(columns={\"size\":\"cnt\"}))\n",
    "    if len(tmp)==0: return pd.DataFrame(columns=[\"acct\"])\n",
    "    wide = tmp.pivot_table(index=\"acct\", columns=col, values=\"cnt\", aggfunc=\"sum\", fill_value=0)\n",
    "    wide.columns = [f\"{prefix}_{str(c)}\" for c in wide.columns]\n",
    "    total = wide.sum(axis=1).replace(0, np.nan)\n",
    "    frac = wide.div(total, axis=0).add_prefix(f\"{prefix}_pct_\")\n",
    "    return pd.concat([wide, frac], axis=1).reset_index()\n",
    "\n",
    "def counterparty_profile(df):\n",
    "    vc = df[\"counterparty\"].value_counts(normalize=True)\n",
    "    if len(vc)==0: return pd.Series({\"ctp_entropy\":0.0,\"ctp_herfindahl\":0.0})\n",
    "    p = vc.values.astype(float)\n",
    "    return pd.Series({\"ctp_entropy\": float(-(p*np.log(p+1e-12)).sum()),\n",
    "                      \"ctp_herfindahl\": float((p**2).sum())})\n",
    "\n",
    "def timebin_profile(df):\n",
    "    vc = df[\"min5_bin\"].value_counts()\n",
    "    if len(vc)==0:\n",
    "        return pd.Series({\"bin5_cnt\":0,\"bin5_top1_ratio\":0.0,\"bin5_entropy\":0.0})\n",
    "    p = (vc/vc.sum()).values.astype(float)\n",
    "    return pd.Series({\"bin5_cnt\": int(vc.shape[0]),\n",
    "                      \"bin5_top1_ratio\": float(p.max()),\n",
    "                      \"bin5_entropy\": float(-(p*np.log(p+1e-12)).sum())})\n",
    "\n",
    "def bucket_profile(df, col):\n",
    "    vc = df[col].value_counts(normalize=True)\n",
    "    if len(vc)==0: return pd.Series({f\"{col}_entropy\":0.0, f\"{col}_top1\":0.0})\n",
    "    p = vc.values.astype(float)\n",
    "    return pd.Series({f\"{col}_entropy\": float(-(p*np.log(p+1e-12)).sum()),\n",
    "                      f\"{col}_top1\": float(p.max())})\n",
    "\n",
    "def build_activity_table(tx_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    if len(tx_long) == 0:\n",
    "        return pd.DataFrame(columns=[\"acct\",\"tx_cnt\",\"uniq_ctp\",\"bin5_entropy\",\"amt_abs_std\"])\n",
    "    g = tx_long.groupby(\"acct\", as_index=False).agg(\n",
    "        tx_cnt=(\"txn_day\",\"size\"),\n",
    "        uniq_ctp=(\"counterparty\", pd.Series.nunique),\n",
    "        amt_abs_std=(\"txn_amt\", lambda s: float(np.std(np.abs(s)))),\n",
    "    )\n",
    "    tb = (tx_long.groupby([\"acct\",\"min5_bin\"], as_index=False).size()\n",
    "          .rename(columns={\"size\":\"cnt\"}))\n",
    "    ent = tb.groupby(\"acct\")[\"cnt\"].apply(lambda s: entropy_from_counts(s.values)).reset_index()\n",
    "    ent.rename(columns={\"cnt\":\"bin5_entropy\"}, inplace=True)\n",
    "    act = g.merge(ent, on=\"acct\", how=\"left\")\n",
    "    act[\"bin5_entropy\"] = act[\"bin5_entropy\"].fillna(0.0)\n",
    "    return act\n",
    "\n",
    "\n",
    "def normalize_numeric_features(X_train: pd.DataFrame, X_pred: pd.DataFrame):\n",
    "    win_cols = [c for c in [\n",
    "        \"tx_cnt\",\"in_cnt\",\"out_cnt\",\"active_days\",\"uniq_ctp\",\n",
    "        \"amt_in_sum\",\"amt_out_sum\",\"amt_abs_mean\",\"amt_abs_std\",\"amt_max\",\"net_flow\",\n",
    "    ] if c in X_train.columns]\n",
    "    win_cols += [c for c in X_train.columns if (c.startswith(\"chan_\") or c.startswith(\"ccy_\")) and not (\"pct_\" in c)]\n",
    "    win_cols = sorted(set(win_cols))\n",
    "    caps = compute_caps(X_train, win_cols, lo_q=0.005, hi_q=0.995)\n",
    "    apply_caps(X_train, caps); apply_caps(X_pred, caps)\n",
    "\n",
    "    log_cols = [c for c in [\n",
    "        \"tx_cnt\",\"in_cnt\",\"out_cnt\",\"active_days\",\"uniq_ctp\",\n",
    "        \"amt_in_sum\",\"amt_out_sum\",\"amt_abs_mean\",\"amt_abs_std\",\"amt_max\",\n",
    "    ] if c in X_train.columns]\n",
    "    signed_log_cols = [c for c in [\"net_flow\"] if c in X_train.columns]\n",
    "    for c in log_cols:\n",
    "        X_train[c] = np.log1p(X_train[c].clip(lower=0)); X_pred[c]  = np.log1p(X_pred[c].clip(lower=0))\n",
    "    for c in signed_log_cols:\n",
    "        X_train[c] = signed_log1p(X_train[c]);         X_pred[c]  = signed_log1p(X_pred[c])\n",
    "    return X_train, X_pred, caps, log_cols, signed_log_cols\n",
    "\n",
    "\n",
    "# =============== 建樣本鍵（含 Hard Negatives） ===============\n",
    "def build_train_keys(tx, alert, tx_long):\n",
    "    # 正類\n",
    "    pos = alert[[\"acct\",\"event_day\"]].dropna().copy()\n",
    "    pos[\"event_day\"] = pos[\"event_day\"].astype(int)\n",
    "    pos = pos.drop_duplicates(\"acct\")\n",
    "    pos[\"label\"] = 1; pos[\"is_unlabeled\"] = 0\n",
    "\n",
    "    # 每帳最後交易日\n",
    "    acc_last = (pd.concat([\n",
    "        tx[[\"from_acct\",\"txn_day\"]].rename(columns={\"from_acct\":\"acct\"}),\n",
    "        tx[[\"to_acct\",\"txn_day\"]].rename(columns={\"to_acct\":\"acct\"})\n",
    "    ], ignore_index=True)\n",
    "      .groupby(\"acct\", as_index=False)[\"txn_day\"].max()\n",
    "      .rename(columns={\"txn_day\":\"last_txn_day\"}))\n",
    "\n",
    "    # U 候選池\n",
    "    tx_accts = set(pd.concat([tx[\"from_acct\"], tx[\"to_acct\"]]).astype(str))\n",
    "    pool = sorted(list(tx_accts - set(pos[\"acct\"])))\n",
    "    neg_full = acc_last[acc_last[\"acct\"].isin(pool)].copy()\n",
    "\n",
    "    if len(neg_full) == 0:\n",
    "        any_accts = pd.Series(list(tx_accts)).sample(n=min(len(pos), len(tx_accts)), random_state=RANDOM_STATE)\n",
    "        fb = pd.DataFrame({\"acct\": any_accts}).merge(acc_last, on=\"acct\", how=\"left\")\n",
    "        fb[\"event_day\"] = fb[\"last_txn_day\"].fillna(tx[\"txn_day\"].max()).astype(int) + 1\n",
    "        neg_full = fb[[\"acct\",\"event_day\"]].copy()\n",
    "    else:\n",
    "        act = build_activity_table(tx_long)  # tx_cnt / uniq_ctp / bin5_entropy / amt_abs_std\n",
    "        neg_full = neg_full.merge(act, on=\"acct\", how=\"left\")\n",
    "        for col in [\"tx_cnt\",\"uniq_ctp\",\"bin5_entropy\",\"amt_abs_std\"]:\n",
    "            neg_full[col] = neg_full[col].fillna(0)\n",
    "        neg_full[\"event_day\"] = neg_full[\"last_txn_day\"] + 1\n",
    "        neg_full = neg_full.sort_values(\n",
    "            [\"tx_cnt\",\"uniq_ctp\",\"bin5_entropy\",\"amt_abs_std\"],\n",
    "            ascending=[False, False, False, False]\n",
    "        )\n",
    "\n",
    "    need = int(max(1, len(pos) * NEG_POS_RATIO))\n",
    "    neg = neg_full.head(min(need, len(neg_full)))[[\"acct\",\"event_day\"]].copy()\n",
    "    neg[\"label\"] = 0; neg[\"is_unlabeled\"] = 1\n",
    "\n",
    "    keys = pd.concat([pos, neg], ignore_index=True).dropna()\n",
    "    keys[\"event_day\"] = keys[\"event_day\"].astype(int)\n",
    "    keys = keys.drop_duplicates(\"acct\")\n",
    "    return keys, acc_last\n",
    "\n",
    "\n",
    "# =============== 主流程 ===============\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--tx\", default=TX_PATH)\n",
    "    ap.add_argument(\"--alert\", default=ALERT_PATH)\n",
    "    ap.add_argument(\"--pred\", default=PRED_PATH)\n",
    "    ap.add_argument(\"--inference_only\", action=\"store_true\",\n",
    "                    help=\"只產生 features_pred.csv（不讀 alert、不輸出 features_train.csv）\")\n",
    "    args, _ = ap.parse_known_args()\n",
    "\n",
    "    tx, alert, pred, tx_long = read_and_normalize(args.tx, args.alert, args.pred, inference_only=args.inference_only)\n",
    "\n",
    "    if not args.inference_only:\n",
    "        train_keys, acc_last = build_train_keys(tx, alert, tx_long)\n",
    "    else:\n",
    "        acc_last = (pd.concat([\n",
    "            tx[[\"from_acct\",\"txn_day\"]].rename(columns={\"from_acct\":\"acct\"}),\n",
    "            tx[[\"to_acct\",\"txn_day\"]].rename(columns={\"to_acct\":\"acct\"})\n",
    "        ], ignore_index=True)\n",
    "          .groupby(\"acct\", as_index=False)[\"txn_day\"].max()\n",
    "          .rename(columns={\"txn_day\":\"last_txn_day\"}))\n",
    "        train_keys = None\n",
    "\n",
    "    # 訓練視窗\n",
    "    if not args.inference_only:\n",
    "        tx_train = tx_long.merge(train_keys[[\"acct\",\"event_day\",\"label\",\"is_unlabeled\"]], on=\"acct\", how=\"inner\")\n",
    "        tx_train[\"delta_days\"] = tx_train[\"event_day\"] - tx_train[\"txn_day\"]\n",
    "        tx_win = tx_train[(tx_train[\"delta_days\"]>=1) & (tx_train[\"delta_days\"]<=WINDOW_DAYS)].copy()\n",
    "        if len(tx_win)==0:\n",
    "            for d in [90]:\n",
    "                tx_win = tx_train[(tx_train[\"delta_days\"]>=1) & (tx_train[\"delta_days\"]<=d)].copy()\n",
    "                if len(tx_win)>0: print(f\"[WARN] 改用 {d} 天窗：{len(tx_win)} 筆\"); break\n",
    "        if len(tx_win)==0:\n",
    "            tx_win = tx_train[tx_train[\"delta_days\"]>=1].copy()\n",
    "            print(f\"[WARN] 改用『事件日前所有歷史』：{len(tx_win)} 筆\")\n",
    "\n",
    "    # 待測視窗（anchor = last+1）\n",
    "    pred_keys = pred[[\"acct\"]].drop_duplicates().merge(\n",
    "        (pd.concat([\n",
    "            tx[[\"from_acct\",\"txn_day\"]].rename(columns={\"from_acct\":\"acct\"}),\n",
    "            tx[[\"to_acct\",\"txn_day\"]].rename(columns={\"to_acct\":\"acct\"})\n",
    "        ], ignore_index=True)\n",
    "          .groupby(\"acct\", as_index=False)[\"txn_day\"].max()\n",
    "          .rename(columns={\"txn_day\":\"last_txn_day\"})),\n",
    "        on=\"acct\", how=\"left\"\n",
    "    )\n",
    "    pred_keys[\"event_day\"] = pred_keys[\"last_txn_day\"].fillna(tx[\"txn_day\"].max()).astype(int)+1\n",
    "\n",
    "    tx_pred = tx_long.merge(pred_keys[[\"acct\",\"event_day\"]], on=\"acct\", how=\"inner\")\n",
    "    tx_pred[\"delta_days\"] = tx_pred[\"event_day\"] - tx_pred[\"txn_day\"]\n",
    "    txp_win = tx_pred[(tx_pred[\"delta_days\"]>=1) & (tx_pred[\"delta_days\"]<=WINDOW_DAYS)].copy()\n",
    "    if len(txp_win)==0:\n",
    "        for d in [90]:\n",
    "            txp_win = tx_pred[(tx_pred[\"delta_days\"]>=1) & (tx_pred[\"delta_days\"]<=d)].copy()\n",
    "            if len(txp_win)>0: print(f\"[WARN] 待測改用 {d} 天窗：{len(txp_win)} 筆\"); break\n",
    "    if len(txp_win)==0:\n",
    "        txp_win = tx_pred[tx_pred[\"delta_days\"]>=1].copy()\n",
    "        print(f\"[WARN] 待測改用『事件日前所有歷史』：{len(txp_win)} 筆\")\n",
    "\n",
    "    # 特徵（訓練）\n",
    "    if not args.inference_only:\n",
    "        feat_base = agg_features(tx_win)\n",
    "        chan_wide = wide_count(tx_win, \"channel_type\", \"chan\")\n",
    "        ccy_wide  = wide_count(tx_win, \"currency_bucket\", \"ccy\")\n",
    "        extra_ctp = safe_group_apply(tx_win.groupby(\"acct\"), counterparty_profile).reset_index()\n",
    "        extra_time= safe_group_apply(tx_win.groupby(\"acct\"), timebin_profile).reset_index()\n",
    "        extra_chan= safe_group_apply(tx_win.groupby(\"acct\"), lambda d: bucket_profile(d,\"channel_type\")).reset_index()\n",
    "        extra_ccy = safe_group_apply(tx_win.groupby(\"acct\"), lambda d: bucket_profile(d,\"currency_bucket\")).reset_index()\n",
    "\n",
    "        X_train = (train_keys[[\"acct\",\"label\",\"is_unlabeled\"]]\n",
    "                   .merge(feat_base, on=\"acct\", how=\"left\")\n",
    "                   .merge(chan_wide, on=\"acct\", how=\"left\")\n",
    "                   .merge(ccy_wide, on=\"acct\", how=\"left\")\n",
    "                   .merge(extra_ctp, on=\"acct\", how=\"left\")\n",
    "                   .merge(extra_time, on=\"acct\", how=\"left\")\n",
    "                   .merge(extra_chan, on=\"acct\", how=\"left\")\n",
    "                   .merge(extra_ccy, on=\"acct\", how=\"left\")\n",
    "                   .fillna(0))\n",
    "\n",
    "    # 特徵（待測）\n",
    "    feat_pred = agg_features(txp_win)\n",
    "    chan_pred = wide_count(txp_win, \"channel_type\", \"chan\")\n",
    "    ccy_pred  = wide_count(txp_win, \"currency_bucket\", \"ccy\")\n",
    "    extra_ctp_p = safe_group_apply(txp_win.groupby(\"acct\"), counterparty_profile).reset_index()\n",
    "    extra_time_p= safe_group_apply(txp_win.groupby(\"acct\"), timebin_profile).reset_index()\n",
    "    extra_chan_p= safe_group_apply(txp_win.groupby(\"acct\"), lambda d: bucket_profile(d,\"channel_type\")).reset_index()\n",
    "    extra_ccy_p = safe_group_apply(txp_win.groupby(\"acct\"), lambda d: bucket_profile(d,\"currency_bucket\")).reset_index()\n",
    "\n",
    "    X_pred = (pred_keys[[\"acct\"]]\n",
    "              .merge(feat_pred, on=\"acct\", how=\"left\")\n",
    "              .merge(chan_pred, on=\"acct\", how=\"left\")\n",
    "              .merge(ccy_pred, on=\"acct\", how=\"left\")\n",
    "              .merge(extra_ctp_p, on=\"acct\", how=\"left\")\n",
    "              .merge(extra_time_p, on=\"acct\", how=\"left\")\n",
    "              .merge(extra_chan_p, on=\"acct\", how=\"left\")\n",
    "              .merge(extra_ccy_p, on=\"acct\", how=\"left\")\n",
    "              .fillna(0))\n",
    "\n",
    "    # 對齊欄位 + 數值正規化 + 輸出\n",
    "    if not args.inference_only:\n",
    "        feature_cols = [c for c in X_train.columns if c not in [\"acct\",\"label\",\"is_unlabeled\"]]\n",
    "        for c in feature_cols:\n",
    "            if c not in X_pred.columns: X_pred[c] = 0\n",
    "        X_pred = X_pred[[\"acct\"] + feature_cols]\n",
    "\n",
    "        X_train, X_pred, caps, log_cols, signed_log_cols = normalize_numeric_features(X_train, X_pred)\n",
    "\n",
    "        X_train.to_csv(OUT_TRAIN, index=False, encoding=\"utf-8-sig\")\n",
    "        with open(OUT_META, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"feature_cols\": feature_cols,\n",
    "                \"window_days\": WINDOW_DAYS,\n",
    "                \"neg_pos_ratio\": NEG_POS_RATIO,\n",
    "                \"caps\": caps,\n",
    "                \"log_cols\": log_cols,\n",
    "                \"signed_log_cols\": signed_log_cols,\n",
    "                \"drift_topk\": DRIFT_TOPK,\n",
    "                \"drift_dropped\": []\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[OK] 保存: {OUT_TRAIN}, {OUT_META}\")\n",
    "        print(\"Train shape:\", X_train.shape, \" Pred shape:\", X_pred.shape)\n",
    "        print(\"[CHECK] label 分佈：\", X_train[\"label\"].value_counts().to_dict())\n",
    "    else:\n",
    "        # inference-only：僅輸出待測特徵與空 meta（提醒不可用於訓練）\n",
    "        feature_cols = [c for c in X_pred.columns if c not in [\"acct\"]]\n",
    "        with open(OUT_META, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"feature_cols\": feature_cols,\n",
    "                \"window_days\": WINDOW_DAYS,\n",
    "                \"neg_pos_ratio\": None,\n",
    "                \"caps\": {},\n",
    "                \"log_cols\": [],\n",
    "                \"signed_log_cols\": [],\n",
    "                \"drift_topk\": 0,\n",
    "                \"drift_dropped\": [],\n",
    "                \"note\": \"inference_only=true；僅供特徵推論，訓練請勿使用此 meta。\"\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[INFO] inference_only 模式：僅寫出 features_pred.csv / features_meta.json（caps/log 皆為空）\")\n",
    "\n",
    "    X_pred.to_csv(OUT_PRED, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[OK] 保存: {OUT_PRED}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
